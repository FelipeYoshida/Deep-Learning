{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4: Funções de Ativação e Processo de Aprendizado em RNAs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação Detalhadas\n",
    "\n",
    "As funções de ativação desempenham um papel fundamental em redes neurais artificiais ao introduzir não-linearidade, permitindo que o modelo aprenda padrões complexos nos dados. Essa não-linearidade é essencial para tarefas como classificação de imagens, reconhecimento de voz, e outras aplicações de aprendizado profundo que requerem o modelo para capturar relações complexas entre os dados de entrada e saída.\n",
    "\n",
    "#### Propósito e Mecanismo\n",
    "- As funções de ativação permitem que as redes neurais superem as limitações dos modelos lineares, introduzindo a capacidade de aprender e representar relações não-lineares entre as entradas e saídas.\n",
    "- Sem não-linearidade, independentemente de quantas camadas uma rede neural possua, ela se comportaria essencialmente como um modelo linear, limitando severamente sua capacidade de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Problema Linear\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z_linear = 2*X + 3*Y\n",
    "\n",
    "# Problema Não Linear\n",
    "Z_nonlinear = np.sin(X) + np.cos(Y)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Gráfico para o problema linear\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z_linear, cmap='viridis')\n",
    "ax1.set_title('Problema Linear')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_zlim(-10, 10)\n",
    "\n",
    "# Gráfico para o problema não linear\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(X, Y, Z_nonlinear, cmap='viridis')\n",
    "ax2.set_title('Problema Não Linear')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação dos Gráficos\n",
    "\n",
    "Os gráficos gerados representam visualizações tridimensionais de funções matemáticas, demonstrando a diferença entre problemas lineares e não lineares.\n",
    "\n",
    "#### Problema Linear\n",
    "No primeiro gráfico, temos uma representação de um problema linear, evidenciado pela equação `Z = 2X + 3Y`. Este é um plano no espaço tridimensional, caracterizado por sua uniformidade e ausência de curvatura. A linearidade indica que as relações entre as variáveis `X`, `Y` e `Z` são diretas, mantendo uma taxa de alteração constante. Isso é típico de sistemas onde a saída é proporcional à entrada, sem elementos de complexidade ou feedback.\n",
    "\n",
    "#### Problema Não Linear\n",
    "O segundo gráfico ilustra um problema não linear, utilizando a equação `Z = sin(X) + cos(Y)`. Aqui, a superfície exibida é claramente curva e varia de acordo com as funções seno e cosseno de `X` e `Y`, respectivamente. A não linearidade é evidente pela maneira como a superfície se dobra e oscila, indicando uma relação complexa entre as variáveis onde a saída não varia proporcionalmente às entradas. Problemas não lineares são comuns em fenômenos naturais e sistemas dinâmicos, caracterizados por comportamentos como caos, limites cíclicos e outros padrões complexos que não podem ser descritos por equações lineares simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de Funções de Ativação\n",
    "\n",
    "- **ReLU (Unidade Linear Retificada):**\n",
    "  - **Uso:** Tornou-se a função de ativação padrão para muitas redes neurais devido à sua simplicidade e eficiência.\n",
    "  - **Vantagens:** Ajuda a acelerar a convergência durante o treinamento e reduz o problema do desvanecimento de gradiente em comparação à Sigmoid ou Tanh.\n",
    "  - **Desvantagens:** Neurônios podem se tornar \"mortos\" durante o treinamento, ou seja, parar de aprender completamente, se mal calibrada.\n",
    "  - **Quando usar:** Na maioria das camadas ocultas de uma rede neural.\n",
    "\n",
    "- **Leaky ReLU:**\n",
    "  - **Uso:** Uma variação da ReLU projetada para mitigar o problema de neurônios mortos, permitindo um pequeno gradiente quando a entrada é menor que zero.\n",
    "  - **Vantagens:** Reduz o problema de neurônios mortos ao permitir um pequeno gradiente negativo.\n",
    "  - **Desvantagens:** A escolha do parâmetro de vazamento (leakiness) pode ser arbitrária e requer ajuste fino.\n",
    "  - **Quando usar:** Como uma alternativa à ReLU, particularmente em redes onde a morte de neurônios se torna um problema.\n",
    "\n",
    "- **Tanh (Tangente Hiperbólica):**\n",
    "  - **Uso:** Similar à função sigmoid, mas gera saídas entre -1 e 1. Isso a torna centralizada em zero e, portanto, em geral, mais eficaz que a sigmoid em camadas ocultas.\n",
    "  - **Vantagens:** Sua natureza centralizada em zero ajuda na eficiência do treinamento.\n",
    "  - **Desvantagens:** Ainda pode sofrer de desvanecimento de gradiente em redes muito profundas.\n",
    "  - **Quando usar:** Em camadas ocultas, especialmente se os dados de entrada também forem centralizados e normalizados.\n",
    "\n",
    "- **Sigmoid:**\n",
    "  - **Uso:** Tradicionalmente usada em classificação binária para produzir uma saída entre 0 e 1, interpretada como probabilidade.\n",
    "  - **Vantagens:** Fácil interpretação dos resultados.\n",
    "  - **Desvantagens:** Suscetível ao desvanecimento de gradiente em redes profundas, o que pode dificultar o treinamento.\n",
    "  - **Quando usar:** Em camadas de saída de problemas de classificação binária.\n",
    "\n",
    "- **Softmax:**\n",
    "  - **Uso:** Comumente usada na camada de saída de redes neurais para tarefas de classificação multiclasse. Ela transforma os logits (valores de entrada brutos de uma camada) em probabilidades, distribuindo-as de forma que a soma de todas as probabilidades de saída seja 1.\n",
    "  - **Vantagens:** A função Softmax é ideal para lidar com múltiplas classes de forma que cada classe receba uma probabilidade proporcional à sua pontuação. Isso facilita a interpretação dos resultados e a decisão de classificação.\n",
    "  - **Desvantagens:** A utilização da Softmax na camada de saída pode levar a problemas de desvanecimento ou explosão do gradiente, especialmente com logits muito altos ou baixos, devido à saturação das funções exponenciais.\n",
    "  - **Quando usar:** Em camadas de saída para problemas de classificação multiclasse, onde é necessário interpretar a saída do modelo como um conjunto de probabilidades.\n",
    "\n",
    "- **Linear:**\n",
    "  - **Uso:** Comumente usada em problemas de regressão onde a saída é um valor contínuo. A função de ativação linear é basicamente a função identidade `f(x) = x`, o que significa que a saída é igual à entrada.\n",
    "  - **Vantagens:** Mantém a linearidade do modelo, o que pode ser vantajoso em tarefas simples de regressão onde não se espera que o relacionamento entre as variáveis seja não linear.\n",
    "  - **Desvantagens:** Limita a rede neural a modelar apenas relações lineares, o que reduz significativamente a capacidade da rede de capturar complexidades nos dados.\n",
    "  - **Quando usar:** Geralmente, é usada na camada de saída de problemas de regressão para permitir a previsão de uma gama contínua de valores sem aplicar uma transformação não linear.\n",
    "\n",
    "Cada função de ativação tem suas peculiaridades e a escolha entre elas depende da arquitetura específica da rede neural e do problema sendo abordado. Experimentar com diferentes funções de ativação e suas configurações é uma prática comum para encontrar o melhor desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as funções de ativação\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtração para evitar overflow\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "def linear(x):\n",
    "    return x \n",
    "\n",
    "# Gerando valores de x\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Plotando as funções de ativação\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x, relu(x), label=\"ReLU\", color=\"green\")\n",
    "plt.title(\"ReLU\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x, leaky_relu(x), label=\"Leaky ReLU\", color=\"purple\")\n",
    "plt.title(\"Leaky ReLU\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x, tanh(x), label=\"Tanh\", color=\"red\")\n",
    "plt.title(\"Tanh\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x, sigmoid(x), label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Para softmax, é necessário mais de um valor de entrada, portanto, consideramos uma dimensão a mais.\n",
    "x_softmax = np.linspace(-3, 3, 100).reshape(1, -1)\n",
    "softmax_vals = softmax(x_softmax).flatten()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x, softmax_vals, label=\"Softmax\", color=\"orange\")\n",
    "plt.title(\"Softmax\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(x, linear(x), label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Linear\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os gráficos bidimensionais apresentam as funções de ativação mais comumente utilizadas em redes neurais: Sigmoid, ReLU, Tanh, Leaky ReLU e Softmax. Cada função tem características únicas que influenciam o aprendizado do modelo de maneiras diferentes.\n",
    "\n",
    "\n",
    "\n",
    "### `ReLU`\n",
    "A função ReLU (Unidade Linear Retificada) é uma função linear para valores positivos e zero para valores negativos. Sua principal vantagem é a capacidade de acelerar a convergência do treinamento em comparação com a Sigmoid e a Tanh, embora possa levar ao problema de neurônios mortos.\n",
    "\n",
    "### `Leaky ReLU`\n",
    "A Leaky ReLU é uma variação da ReLU que permite um pequeno gradiente para valores negativos, reduzindo o problema de neurônios mortos. Isso é conseguido introduzindo um pequeno coeficiente para valores negativos.\n",
    "\n",
    "### `Tanh`\n",
    "A função Tanh (Tangente Hiperbólica) é semelhante à Sigmoid, mas produz saídas que variam de -1 a 1. Isso a torna mais eficaz que a Sigmoid em camadas ocultas, especialmente quando os dados estão centralizados em torno de zero.\n",
    "\n",
    "### `Sigmoid`\n",
    "A função Sigmoid é suave e produz uma saída entre 0 e 1, o que a torna adequada para problemas de classificação binária e para interpretar a saída como probabilidade. No entanto, ela pode sofrer de desvanecimento do gradiente em redes profundas.\n",
    "\n",
    "### `Softmax`\n",
    "A função Softmax é utilizada na camada de saída de redes neurais para tarefas de classificação multiclasse. Ela converte os logits (valores de entrada brutos) em probabilidades para cada classe, garantindo que a soma de todas as probabilidades seja 1.\n",
    "\n",
    "### `Linear`\n",
    "A função Linear é essencialmente a função de ativação mais simples, onde a saída é igual à entrada. É frequentemente usada em camadas de saída para problemas de regressão, onde a não-linearidade não é desejada. Sua principal característica é não alterar os dados de entrada, mantendo uma relação direta e proporcional entre a entrada e a saída, o que facilita a interpretação dos resultados em tarefas como previsão de valores contínuos.\n",
    "\n",
    "Cada função de ativação tem seu papel e utilidade, dependendo do tipo de problema que está sendo resolvido e da arquitetura da rede neural em questão. A escolha da função de ativação adequada pode impactar significativamente o desempenho e a eficácia do modelo de aprendizado de máquina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Classificação Binária (ex: Detecção de Spam)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Estrutura\n",
    "model_bin = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(100,)),  # Camada oculta com ReLU, adequada para não-linearidade\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída com Sigmoid para classificação binária (Spam ou Não Spam)\n",
    "])\n",
    "# ReLU é escolhida por sua eficiência em adicionar não-linearidade sem sofrer do problema do gradiente desvanecente.\n",
    "# Sigmoid é usada na saída para obter uma probabilidade entre 0 e 1.\n",
    "\n",
    "# Exemplo 2: Classificação Multiclasse (ex: Reconhecimento de Dígitos)\n",
    "model_multi = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),  # Camada oculta com ReLU\n",
    "    Dense(10, activation='softmax')  # Camada de saída com Softmax para classificação de 10 classes (dígitos de 0 a 9)\n",
    "])\n",
    "# Softmax na camada de saída distribui a probabilidade entre várias classes, ideal para classificação multiclasse.\n",
    "\n",
    "# Exemplo 3: Regressão (ex: Previsão do Preço de Casas)\n",
    "model_reg = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(13,)),  # Camada oculta com ReLU\n",
    "    Dense(1)  # Camada de saída sem função de ativação para regressão (preço da casa)\n",
    "])\n",
    "# Na regressão, a camada de saída geralmente não possui função de ativação para permitir a previsão de valores contínuos.\n",
    "\n",
    "# Exemplo 4: Processamento de Linguagem Natural (ex: Análise de Sentimento)\n",
    "model_nlp = Sequential([\n",
    "    Dense(64, activation='tanh', input_shape=(1000,)),  # Camada oculta com Tanh\n",
    "    Dense(1, activation='sigmoid')  # Classificação binária (sentimento positivo ou negativo)\n",
    "])\n",
    "# Tanh é escolhida pela sua natureza centralizada em zero, o que pode ser benéfico em algumas estruturas de dados de texto.\n",
    "\n",
    "# Exemplo 5: Reconhecimento de Imagens com Camadas Leaky ReLU\n",
    "model_img_leaky = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(1024,)),  # Primeira camada oculta com ReLU\n",
    "    Dense(64, activation='leaky_relu'),  # Segunda camada oculta com Leaky ReLU\n",
    "    Dense(32, activation='leaky_relu'),  # terceira camada oculta com Leaky ReLU\n",
    "    Dense(10, activation='softmax')  # Classificação multiclasse com Softmax\n",
    "])\n",
    "# Leaky ReLU é usada para mitigar o problema de neurônios mortos que podem ocorrer com ReLU.\n",
    "\n",
    "# Exemplo 6: Modelo Generativo (ex: Geração de Texto)\n",
    "model_gen = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(100,)),  # Camada oculta com ReLU\n",
    "    Dense(5000, activation='softmax')  # Camada de saída com Softmax para gerar a próxima palavra em um vocabulário de 5000 palavras\n",
    "])\n",
    "# Softmax é usada para selecionar a próxima palavra com base em uma distribuição de probabilidade.\n",
    "\n",
    "# Exemplo 7: Detecção de Anomalias em Séries Temporais\n",
    "model_anom = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(30,)),  # Camada oculta com ReLU\n",
    "    Dense(1, activation='sigmoid')  # Detecção binária de anomalia (normal ou anômalo)\n",
    "])\n",
    "\n",
    "# Exemplo 8: Sistema de Recomendação\n",
    "model_rec = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(50,)),  # Camada oculta com ReLU\n",
    "    Dense(10, activation='softmax')  # Camada de saída com Softmax para recomendar um dos 10 produtos\n",
    "])\n",
    "\n",
    "# Exemplo 9: Classificação de Sentimentos em Tweets\n",
    "model_tweet = Sequential([\n",
    "    Dense(128, activation='tanh', input_shape=(300,)),  # Camada oculta com Tanh\n",
    "    Dense(64, activation='tanh'),  # Camada oculta com Tanh\n",
    "    Dense(32, activation='tanh'),  # Camada oculta com Tanh\n",
    "    Dense(16, activation='tanh'),  # Camada oculta com Tanh\n",
    "    Dense(3, activation='softmax')  # Classificação multiclasse (positivo, neutro, negativo)\n",
    "])\n",
    "\n",
    "# Exemplo 10: Previsão de Séries Temporais (ex: Ações)\n",
    "model_stock = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(60,)),  # Últimos 60 dias como entrada, usando ReLU para camada oculta\n",
    "    Dense(1)  # Previsão do preço da ação no próximo dia (valor contínuo, sem função de ativação específica)\n",
    "])\n",
    "# Neste exemplo de regressão para previsão de séries temporais, a última camada é linear, permitindo a previsão de um valor contínuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de Aprendizado\n",
    "\n",
    "O processo de aprendizado em redes neurais é fundamentalmente sobre o ajuste iterativo de pesos e biases, visando minimizar a diferença entre as previsões do modelo e os valores reais. Esse processo envolve conceitos-chave como perda, custo e otimização, cada um desempenhando um papel crucial na eficácia do treinamento da rede.\n",
    "\n",
    "#### Ajuste de Pesos e Bias\n",
    "- **Mecanismo:** A cada iteração de treinamento, o modelo ajusta seus pesos e biases com base no gradiente da função de perda, um processo conhecido como retropropagação (``backpropagation``).\n",
    "- **Objetivo:** O objetivo é reduzir o erro nas previsões do modelo, ajustando os pesos (a importância dada a cada característica de entrada) e os biases (valores de ajuste que permitem que o modelo se ajuste melhor aos dados).\n",
    "\n",
    "\n",
    "### Função de Perda e Gradiente da Função de Perda\n",
    "\n",
    "A otimização de redes neurais envolve minimizar uma função de perda, também conhecida como função de custo, que mede a discrepância entre as previsões do modelo e os valores reais dos dados. O gradiente da função de perda desempenha um papel crucial neste processo, orientando a atualização dos pesos da rede para melhorar seu desempenho.\n",
    "\n",
    "### Função de Perda\n",
    "\n",
    "- **Definição:** A função de perda quantifica a diferença entre a saída prevista pelo modelo e o valor real esperado. Em outras palavras, mede o \"erro\" do modelo.\n",
    "- **Objetivo:** O principal objetivo durante o treinamento de uma rede neural é minimizar a função de perda, ou seja, reduzir o erro do modelo.\n",
    "- **Tipos Comuns:**\n",
    "  - **Erro Quadrático Médio (MSE):** Usado principalmente em problemas de regressão.\n",
    "  - **Entropia Cruzada:** Usada em problemas de classificação, tanto binária (Binary Cross-Entropy) quanto multiclasse (Categorical Cross-Entropy).\n",
    "\n",
    "### Gradiente da Função de Perda\n",
    "\n",
    "- **Definição:** O gradiente de uma função de perda em relação aos pesos da rede indica a direção na qual os pesos devem ser ajustados para minimizar a perda.\n",
    "- **Cálculo:** O gradiente é calculado através da derivada da função de perda em relação a cada peso, o que mostra como uma pequena mudança em um peso afeta o valor da função de perda.\n",
    "- **Utilização no Treinamento:**\n",
    "  - **Retropropagação:** O processo de ajustar os pesos da rede com base no gradiente da função de perda é conhecido como retropropagação. Utilizando algoritmos de otimização (como SGD, Adam), os pesos são atualizados na direção oposta ao gradiente para alcançar a minimização da perda.\n",
    "\n",
    "### Importância do Gradiente\n",
    "\n",
    "- **Direção e Magnitude:** O gradiente não apenas indica a direção na qual a função de perda aumenta ou diminui, mas também a velocidade dessa mudança (magnitude do gradiente).\n",
    "- **Ponto de Mínimo:** Em um mínimo local ou global, o gradiente da função de perda é zero, indicando que pequenas variações nos pesos não melhoram nem pioram significativamente o desempenho do modelo.\n",
    "\n",
    "### Desafios\n",
    "\n",
    "- **Mínimos Locais vs. Global:** Um dos principais desafios na otimização de redes neurais é evitar ficar preso em mínimos locais enquanto se busca o mínimo global, que representa a solução ótima.\n",
    "- **Desvanecimento/Explosão dos Gradientes:** Grandes variações nos valores do gradiente podem causar instabilidade no treinamento, conhecidas como problemas de desvanecimento ou explosão dos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulação de uma função de perda complexa com múltiplos mínimos locais e um mínimo global distinto\n",
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.sin(x)**2 + 0.1*(x**2)\n",
    "\n",
    "# Calculando o gradiente da função de perda manualmente para simplificar\n",
    "grad_y = 2*np.sin(x)*np.cos(x) + 0.2*x\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotando a função de perda com múltiplos mínimos locais e um mínimo global\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, label=\"Função de Perda\")\n",
    "min_global_x = x[np.argmin(y)]\n",
    "min_global_y = np.min(y)\n",
    "plt.scatter(min_global_x, min_global_y, color='red', label=\"Mínimo Global\")\n",
    "plt.title(\"Função de Perda: Mínimos Locais e Global\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.legend()\n",
    "\n",
    "# Plotando o gradiente da função de perda\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, grad_y, label=\"Gradiente da Função de Perda\")\n",
    "plt.title(\"Gradiente da Função de Perda\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f'(x)$\")\n",
    "plt.axhline(0, color='gray', lw=0.5)  # Linha y=0 para referência\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Perda e Custo em Profundidade\n",
    "- **Perda (Loss):**\n",
    "  - **Definição:** A perda é calculada como a diferença entre a saída prevista pelo modelo e a saída real/verdadeira.\n",
    "  - **Funções de Perda Comuns:** \n",
    "    - **MSE (Mean Squared Error):** Usado em problemas de regressão, calcula o quadrado da diferença entre previsões e valores reais.\n",
    "    - **Cross-Entropy:** Usado em classificação, mede a diferença entre duas distribuições de probabilidade para a variável de saída.\n",
    "- **Custo (Cost):**\n",
    "  - **Definição:** O custo é a agregação (geralmente a média) das perdas calculadas para todo o conjunto de dados de treinamento.\n",
    "  - **Importância:** Fornece uma medida abrangente de quão bem o modelo está performando sobre todo o conjunto de dados.\n",
    "\n",
    "#### Otimização e Taxa de Aprendizado\n",
    "- **SGD (Stochastic Gradient Descent):**\n",
    "  - **Características:** Simples e eficaz, mas pode sofrer de alta variabilidade, o que pode tornar o treinamento inconsistente.\n",
    "  - **Funcionamento:** Ajusta os pesos em pequenos passos, na direção que minimiza a função de custo, com cada passo proporcional ao negativo do gradiente.\n",
    "- **Adam (Adaptive Moment Estimation):**\n",
    "  - **Características:** Combina as técnicas de AdaGrad e RMSProp para ajustar a taxa de aprendizado de cada peso individualmente, o que pode levar a melhores resultados.\n",
    "  - **Funcionamento:** Mantém uma taxa de aprendizado mais eficiente por adaptar o tamanho dos passos do gradiente para cada parâmetro.\n",
    "- **Decaimento da Taxa de Aprendizado:**\n",
    "  - **Definição:** Uma técnica para diminuir a taxa de aprendizado ao longo do tempo.\n",
    "  - **Benefícios:** Ajuda a assegurar uma convergência mais suave para o mínimo global, evitando passos grandes demais que podem levar a oscilações ou a uma convergência prematura em mínimos locais.\n",
    "\n",
    "Este processo de aprendizado é iterativo e requer ajustes finos de parâmetros, como a taxa de aprendizado, para alcançar a melhor performance do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Exemplo 1: Rede Neural para Regressão\n",
    "model_regress = Sequential([\n",
    "    Input(shape=(30,)),  # Definindo explicitamente a camada de entrada com 30 features\n",
    "    Dense(64, activation='relu'),  # Primeira camada oculta com 64 neurônios e ativação ReLU\n",
    "    Dense(64, activation='relu'),  # Segunda camada oculta\n",
    "    Dense(64, activation='relu'),  # Terceira camada oculta\n",
    "    Dense(64, activation='relu'),  # Quarta camada oculta\n",
    "    Dense(1)  # Camada de saída para regressão, sem função de ativação para prever valores contínuos\n",
    "])\n",
    "# Compilando o modelo\n",
    "model_regress.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='mean_squared_error')  # Usando MSE como função de perda para regressão\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo 2: Rede Neural para Classificação Binária\n",
    "model_bin_class = Sequential([\n",
    "    Input(shape=(30,)),\n",
    "    Dense(64, activation='relu'),  # Utilizando ReLU\n",
    "    Dense(64, activation='leaky_relu'),  # Alternando para Leaky ReLU\n",
    "    Dense(64, activation='relu'),  # Voltando para ReLU\n",
    "    Dense(64, activation='leaky_relu'),  # E Leaky ReLU novamente\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída com Sigmoid para classificação binária\n",
    "])\n",
    "# Compilando o modelo\n",
    "model_bin_class.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                        loss='binary_crossentropy',  # Usando entropia cruzada binária\n",
    "                        metrics=['accuracy'])  # Métrica de acurácia para avaliação\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo 3: Rede Neural para Classificação de 4 Classes\n",
    "model_multi_class = Sequential([\n",
    "    Input(shape=(30,)),\n",
    "    Dense(64, activation='tanh'),  # Usando Tanh em todas as camadas ocultas\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(4, activation='softmax')  # Camada de saída com Softmax para 4 classes\n",
    "])\n",
    "# Compilando o modelo\n",
    "model_multi_class.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                          loss='categorical_crossentropy',  # Usando entropia cruzada categórica\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Callbacks para todos os modelos\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.000001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Exemplo de treinamento (aplicável a qualquer modelo acima)\n",
    "# model.fit(x_train, y_train, validation_split=0.2, epochs=100, callbacks=[reduce_lr, early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação e Análise de Funções de Ativação\n",
    "\n",
    "A escolha da função de ativação é uma das decisões mais importantes no projeto de redes neurais, pois influencia diretamente a capacidade do modelo de aprender padrões complexos. O TensorFlow e o Keras fornecem um ambiente robusto para experimentar diferentes funções de ativação, facilitando a comparação de seu impacto no desempenho do modelo. \n",
    "\n",
    "#### Uso de TensorFlow/Keras\n",
    "\n",
    "TensorFlow, com sua interface de alto nível Keras, é uma poderosa biblioteca de aprendizado de máquina que simplifica a construção, treinamento e avaliação de modelos de redes neurais. Para explorar o impacto das funções de ativação, siga os passos detalhados abaixo:\n",
    "\n",
    "1. **Configuração do Ambiente:**\n",
    "   - Certifique-se de ter o TensorFlow instalado no seu ambiente de desenvolvimento. Você pode instalá-lo via pip com `pip install tensorflow` ou usar o ambiente do google colab.\n",
    "\n",
    "2. **Construção da Rede Neural:**\n",
    "   - Inicie importando os pacotes necessários do Keras: `from tensorflow.keras.models import Sequential` e `from tensorflow.keras.layers import Dense`.\n",
    "   - Crie um modelo sequencial, que permite construir a rede camada por camada: `model = Sequential()`.\n",
    "\n",
    "3. **Adicionando Camadas com Funções de Ativação:**\n",
    "   - Adicione camadas à sua rede usando o método `add()`. Por exemplo, para adicionar uma camada densa (totalmente conectada) com uma função de ativação ReLU, você usaria: `model.add(Dense(64, activation='relu'))`.\n",
    "   - Repita o processo para experimentar com diferentes funções de ativação para as camadas ocultas.\n",
    "\n",
    "4. **Compilação do Modelo:**\n",
    "   - Compile o modelo com um otimizador, uma função de perda e métricas para avaliação: `model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])`.\n",
    "\n",
    "5. **Treinamento e Avaliação:**\n",
    "   - Treine o modelo usando `model.fit()`, passando seus dados de treinamento, número de épocas e, opcionalmente, dados de validação.\n",
    "   - Avalie o desempenho do modelo no conjunto de teste com `model.evaluate()`.\n",
    "\n",
    "6. **Comparação de Funções de Ativação:**\n",
    "   - Após treinar modelos separados com diferentes funções de ativação, compare os resultados de precisão e perda para determinar qual função oferece o melhor desempenho para o seu problema específico.\n",
    "\n",
    "#### Análise de Resultados\n",
    "\n",
    "- **Interpretação:** Analise os resultados, prestando atenção à precisão e à perda nos conjuntos de treinamento e teste. Funções de ativação diferentes podem levar a resultados distintos, dependendo da natureza do problema e da arquitetura da rede.\n",
    "- **Decisão:** Escolha a função de ativação que proporciona um equilíbrio ideal entre rapidez no treinamento, precisão e capacidade de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Atributos em Aprendizado de Máquina\n",
    "\n",
    "A engenharia de atributos é um processo fundamental no desenvolvimento de modelos de aprendizado de máquina, que envolve a criação de novos atributos (features) a partir dos dados existentes para melhorar o desempenho do modelo. Este processo não se limita apenas à limpeza e preparação dos dados, como a remoção de valores nulos ou tratamento de outliers, mas estende-se à transformação e criação de novos dados que podem revelar padrões importantes não capturados pelos atributos originais.\n",
    "\n",
    "## Importância da Engenharia de Atributos\n",
    "\n",
    "- **Melhoria da Performance do Modelo:** Atributos bem projetados podem significativamente melhorar a capacidade do modelo de aprender padrões complexos, resultando em melhores previsões ou classificações.\n",
    "- **Redução da Complexidade do Modelo:** Atributos mais informativos podem reduzir a necessidade de modelos excessivamente complexos, o que, por sua vez, pode diminuir o risco de overfitting.\n",
    "- **Insights de Domínio:** A criação de novos atributos frequentemente requer um conhecimento profundo do domínio do problema, o que pode levar à descoberta de insights valiosos sobre os dados.\n",
    "\n",
    "## Exemplos de Técnicas de Engenharia de Atributos\n",
    "\n",
    "- **Binning:** Transformação de variáveis contínuas em categóricas, agrupando-os em diferentes 'bins' ou categorias.\n",
    "- **One-Hot Encoding:** Transformação de variáveis categóricas em um formato que possa ser fornecido aos algoritmos de ML, criando uma coluna binária para cada categoria.\n",
    "- **Feature Interaction:** Criação de novos atributos que são combinações de dois ou mais atributos existentes, capturando interações entre eles que podem ser relevantes para a tarefa de predição.\n",
    "- **Polynomial Features:** Geração de atributos que são potências ou combinações polinomiais dos atributos existentes, útil em modelos lineares para capturar relações não lineares.\n",
    "- **Extração de Atributos de Data/Hora:** Transformação de timestamps em componentes mais úteis, como hora do dia, dia da semana, mês, ou até mesmo feriados.\n",
    "\n",
    "## Desafios na Engenharia de Atributos\n",
    "\n",
    "- **Dimensionalidade:** A criação de um grande número de atributos pode levar à alta dimensionalidade, o que pode aumentar a complexidade do modelo e o risco de overfitting.\n",
    "- **Relevância:** Nem todos os atributos criados serão úteis para o modelo, exigindo técnicas de seleção de atributos para identificar aqueles que realmente contribuem para o poder preditivo.\n",
    "- **Custo Computacional:** O processo de criação e teste de novos atributos pode ser computacionalmente intensivo, especialmente com grandes volumes de dados.\n",
    "\n",
    "Em resumo, a engenharia de atributos é uma etapa crucial no processo de modelagem em aprendizado de máquina, requerendo criatividade, conhecimento de domínio e experimentação cuidadosa para melhorar a qualidade e a eficácia dos modelos preditivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('uber.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as colunas irrelevantes\n",
    "df.drop(['index','id'], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a linha que possuia valores nulos\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo a coluna 'data_hora' de string para datetime, removendo ' UTC'\n",
    "df['data_hora'] = pd.to_datetime(df['data_hora'].str[:-4])\n",
    "\n",
    "# Criando colunas adicionais com informações de dia do mês, dia da semana, dia do ano, hora e minuto\n",
    "df['dia_do_mes'] = df['data_hora'].dt.day\n",
    "df['dia_da_semana'] = df['data_hora'].dt.dayofweek  # Segunda é 0 e Domingo é 6\n",
    "df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
    "df['hora'] = df['data_hora'].dt.hour\n",
    "df['minuto'] = df['data_hora'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calcula a distância do círculo grande entre dois pontos \n",
    "    na Terra (especificados em graus decimais).\n",
    "    \"\"\"\n",
    "    # Converte graus decimais para radianos\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Fórmula de Haversine\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Raio da Terra em quilômetros\n",
    "    return c * r\n",
    "\n",
    "# Descobrir a Distância em linha reta em quilometros entre o Ponto de embarque e de desembarque\n",
    "df['distance'] = df.apply(lambda row: haversine(row['pickup_longitude'], row['pickup_latitude'], \n",
    "                                                row['dropoff_longitude'], row['dropoff_latitude']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Separar X e y\n",
    "X = df.drop(['valor','data_hora'], axis = 1)\n",
    "y = df[['valor']]\n",
    "\n",
    "# Separa X e y de treinamento (50%) validação (30%) e teste (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=81)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=6/10, random_state=81)\n",
    "\n",
    "# Normaliza os dados de X\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_val = scaler_x.transform(X_val)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "\n",
    "# Normaliza os dados de y (para regressão)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Maior Valor de y de Treinamento {y_train.max()}')\n",
    "print(f'Menor Valor de y de Treinamento {y_train.min()}')\n",
    "print(f'Maior Valor de y de Validação {y_val.max()}')\n",
    "print(f'Menor Valor de y de Validação {y_val.min()}')\n",
    "print(f'Maior Valor de y de Teste {y_test.max()}')\n",
    "print(f'Menor Valor de y de Teste {y_test.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Definindo a arquitetura da rede neural\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Camada de entrada e primeira camada oculta\n",
    "    Dense(64, activation='relu'),  # Segunda camada oculta\n",
    "    Dense(32, activation='relu'),  # Terceira camada oculta\n",
    "    Dense(1)  # Camada de saída\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=15, batch_size=64,\n",
    "                    callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o histórico de treinamento\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extrai as métricas do histórico\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotando a loss do treinamento e da validação\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Desnormalizando as previsões e os valores de y_test\n",
    "y_pred_desnorm = scaler_y.inverse_transform(y_pred)\n",
    "y_test_desnorm = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Calculando métricas de regressão com os dados desnormalizados\n",
    "mse = mean_squared_error(y_test_desnorm, y_pred_desnorm)\n",
    "mae = mean_absolute_error(y_test_desnorm, y_pred_desnorm)\n",
    "r2 = r2_score(y_test_desnorm, y_pred_desnorm)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R^2: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que 'y_pred_desnorm' e 'y_test_desnorm' sejam os arrays com os valores preditos e reais desnormalizados\n",
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame com os valores preditos e reais\n",
    "df_results = pd.DataFrame({'Valores Reais': y_test_desnorm.flatten(), 'Valores Preditos': y_pred_desnorm.flatten()})\n",
    "df_results = df_results.sample(frac=1, random_state=70)\n",
    "df_results = df_results[:500]\n",
    "# Ordenando o DataFrame pelos valores reais\n",
    "df_results_sorted = df_results.sort_values('Valores Reais').reset_index(drop=True)\n",
    "\n",
    "# Plotando os valores reais e os preditos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_results_sorted.index, df_results_sorted['Valores Reais'], label='Valores Reais', color='blue')\n",
    "plt.plot(df_results_sorted.index, df_results_sorted['Valores Preditos'], label='Valores Preditos', color='red', linestyle='--')\n",
    "plt.title('Comparação entre Valores Reais e Preditos')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.qtd_passageiros.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar X e y\n",
    "X = df.drop(['qtd_passageiros','data_hora'], axis = 1)\n",
    "y = df[['qtd_passageiros']]\n",
    "\n",
    "# Separa X e y de treinamento (50%) validação (30%) e teste (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=81)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=6/10, random_state=81)\n",
    "\n",
    "# Na Classificação Normaliza Apenas os dados de X\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_val = scaler_x.transform(X_val)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "\n",
    "# Convertendo o y para categórico\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a rede neural\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')  # O número de neurônios corresponde ao número de classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "history = model.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat),\n",
    "                    epochs=15, batch_size=64, callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando os dados de histórico\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotando a acurácia de treinamento e validação\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotando o loss de treinamento e validação\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confusão\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Plotando a matriz de confusão com gráfico de calor\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.ylabel('Valores Reais')\n",
    "plt.xlabel('Valores Preditos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios Valendo um UPGRADE na nota da primeira prova\n",
    "\n",
    "1 - Leia a base de dados 'preco_passagem.csv'\n",
    "\n",
    "2 - crie uma coluna que calcula quantos dias tem de diferença entre a data da compra e a data do voo\n",
    "\n",
    "3 - crie uma coluna com o nome da empresa aérea (primeira parte da coluna Airline-Class)\n",
    "\n",
    "4 - crie uma coluna com o nome da classe da passagem (terceira parte da coluna Airline-Class)\n",
    "\n",
    "5 - Crie uma Coluna com a hora da partida (apenas a hora, desconsiderar os minutos)\n",
    "\n",
    "6 - Crie uma Coluna com a hora da chegada (apenas a hora, desconsiderar os minutos)\n",
    "\n",
    "7 - Crie uma coluna de tempo_viagem_minutos com o total de minutos da viagem\n",
    "\n",
    "8 - Crie uma coluna qtd_paradas com as quantias numéricas de paradas (conexões que o voo faz até o destino final) (0 se for voo direto, 1 se for com uma parada ...)\n",
    "\n",
    "9 - A coluna valor é a classe alvo, mas ela veio com separador de milhar. Arrume para que a ela possua apenas numeros inteiros\n",
    "\n",
    "10 - Separe os dados em X e y, sendo X todas as colunas que você criou e y a coluna valor\n",
    "\n",
    "11 - Separe os dados em X e y de treinamento (50%), validação (20%) e teste (30%)\n",
    "\n",
    "12 - Normalize os valores de X e y de treinamento, validação e teste\n",
    "\n",
    "13 - Defina uma rede neural para regressão com no mínimo 3 camadas ocultas\n",
    "\n",
    "14 - Defina a paciência da redução da taxa de aprendizado em 5 e da parada antecipada do treinamento em 10 épocas\n",
    "\n",
    "15 - Coloque o modelo para treinar por 100 épocas com os callbacks de ReduceLROnPlateau e EarlyStopping definidos\n",
    "\n",
    "16 - Exiba um gráfico com o o Loss nos dados de treinamento e validação ao longo das épocas\n",
    "\n",
    "17 - Exiba as métricas de MSE, MAE e Score R².\n",
    "\n",
    "18 - Faça um gráfico exibindo a comparação dos valores reais e preditos nos dados de teste (usando apenas 500 amostras do total)\n",
    "\n",
    "\n",
    "# REQUISITOS PARA GANHAR O UPGRADE NA NOTA\n",
    "\n",
    "Para ter direito ao upgrade na nota, siga atentamente as instruções abaixo. Estas regras são projetadas para garantir que todos tenham uma chance justa e demonstrem seu conhecimento e habilidades de maneira eficaz.\n",
    "\n",
    "- **Início do Exercício:**\n",
    "  - Todos os alunos devem iniciar o exercício **durante a aula**. É importante demonstrar comprometimento e aproveitar o tempo em sala para esclarecer dúvidas e progredir no exercício.\n",
    "\n",
    "- **Apresentação do Notebook:**\n",
    "  - O notebook contendo todas as soluções, incluindo saídas como métricas e gráficos, deve ser apresentado **exclusivamente na aula do dia 19/03/2024**.\n",
    "  - A apresentação será realizada **somente no horário de 08:00 às 08:35**. Certifique-se de estar preparado(a) e disponível neste período para mostrar seu trabalho.\n",
    "\n",
    "- **Conhecimento do Código:**\n",
    "  - Se perguntado sobre qualquer parte do código durante sua apresentação, você deve ser capaz de explicar seu funcionamento e propósito claramente.\n",
    "  - A incapacidade de responder às perguntas sobre o código que você apresenta resultará na **não consideração do upgrade na nota**. É fundamental que você entenda completamente o trabalho que está apresentando.\n",
    "\n",
    "- **Estrutura do Arquivo:**\n",
    "  - Durante o desenvolvimento do exercício, é permitido criar blocos de código adicionais para testes. No entanto, ao preparar o notebook para apresentação, **não altere a estrutura original** do arquivo fornecido. Os enunciados (blocos de markdown - texto) e os blocos de código fornecidos devem permanecer intactos.\n",
    "  - **Não exclua** nenhum bloco de código ou de texto existente e **não adicione** novos blocos de código além do que foi fornecido para cada questão. Para a entrega, cada questão deve ser respondida em seu respectivo bloco de código único designado para ela.\n",
    "\n",
    "Cumprir estes requisitos demonstrará não apenas sua habilidade técnica, mas também seu comprometimento e preparação para a aula. Boa sorte a todos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Importe as bibliotecas aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Leia a base de dados 'preco_passagem.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 - crie uma coluna que calcula quantos dias tem de diferença entre a data da compra e a data do voo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3 - crie uma coluna com o nome da empresa aérea (primeira parte da coluna Airline-Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 - crie uma coluna com o nome da classe da passagem (terceira parte da coluna Airline-Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5 - Crie uma Coluna com a hora da partida (apenas a hora, desconsiderar os minutos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6 - Crie uma Coluna com a hora da chegada (apenas a hora, desconsiderar os minutos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7 - Crie uma coluna de tempo_viagem_minutos com o total de minutos da viagem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8 - Crie uma coluna qtd_paradas com as quantias numéricas de paradas (conexões que o voo faz até o destino final) (0 se for voo direto, 1 se for com uma parada ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 9 - A coluna valor é a classe alvo, mas ela veio com separador de milhar. Arrume para que a ela possua apenas numeros inteiros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10 - Separe os dados em X e y, sendo X todas as colunas que você criou e y a coluna valor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11 - Separe os dados em X e y de treinamento (50%), validação (20%) e teste (30%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 12 - Normalize os valores de X e y de treinamento, validação e teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 13 - Defina uma rede neural para regressão com no mínimo 3 camadas ocultas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 14 - Defina a paciência da redução da taxa de aprendizado em 5 e da parada antecipada do treinamento em 10 épocas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 15 - Coloque o modelo para treinar por 100 épocas com os callbacks de ReduceLROnPlateau e EarlyStopping definidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 16 - Exiba um gráfico com o o Loss nos dados de treinamento e validação ao longo das épocas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 17 - Exiba as métricas de MSE, MAE e Score R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 18 - Faça um gráfico exibindo a comparação dos valores reais e preditos nos dados de teste (usando apenas 500 amostras do total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenvolva sei código aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
